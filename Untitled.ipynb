{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8516b2e5-4d45-4881-ac56-9b25891c5e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bs4) (4.14.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->bs4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7208b1e2-8a6c-4524-b93a-3061a5222b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ixml\n",
      "  Downloading ixml-0.1.0.zip (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: ixml\n",
      "  Building wheel for ixml (pyproject.toml): started\n",
      "  Building wheel for ixml (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for ixml: filename=ixml-0.1.0-py3-none-any.whl size=10482 sha256=e12bad264a38ff621fb80e3d71375f68e459e45f787ba4a3ed4ff18adce13e02\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\74\\7b\\2b\\2a0954a2b147055dfb24b30d6f2cba5e64b56fc5d64f93c701\n",
      "Successfully built ixml\n",
      "Installing collected packages: ixml\n",
      "Successfully installed ixml-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ixml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8d2570-8597-490c-8502-8e0fb2b7b06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af14d582-71b8-4950-9b15-ea390e6b45d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m webpage = requests.get(URL, headers=HEADERS)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Parse with BeautifulSoup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or \"html.parser\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Example: scrape all <h2> tags\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h2 \u001b[38;5;129;01min\u001b[39;00m soup.find_all(\u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\bs4\\__init__.py:366\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    367\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    370\u001b[39m         )\n\u001b[32m    371\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# URL to scrape\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&dc&qid=1760524556&rnid=8609959031&ref=sr_nr_p_n_condition-type_1&ds=v1%3At9rpJ7fSOGDjM43DA7tb2CR74smyug4Rh2omZo5%2BJJk\" # <-- replace with your target URL\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(\"out.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write header row if file is empty\n",
    "    # writer.writerow([\"Column1\", \"Column2\"])  # <-- customize columns\n",
    "\n",
    "    # Headers for request (User-Agent + Accept-Language)\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' +\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) ' +\n",
    "                      'Chrome/115.0.5790.98 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")  # or \"html.parser\"\n",
    "\n",
    "    # Example: scrape all <h2> tags\n",
    "    for h2 in soup.find_all(\"h2\"):\n",
    "        text = h2.get_text(strip=True)\n",
    "        print(text)\n",
    "        writer.writerow([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e9fb96-ee2a-444c-b6b4-2c0eb21529d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m webpage = requests.get(URL, headers=HEADERS)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Parse with BeautifulSoup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or \"html.parser\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Example: scrape all <h2> tags\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h2 \u001b[38;5;129;01min\u001b[39;00m soup.find_all(\u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\bs4\\__init__.py:366\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    367\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    370\u001b[39m         )\n\u001b[32m    371\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# URL to scrape\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&dc&qid=1760524556&rnid=8609959031&ref=sr_nr_p_n_condition-type_1&ds=v1%3At9rpJ7fSOGDjM43DA7tb2CR74smyug4Rh2omZo5%2BJJk\" # <-- replace with your target URL\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(\"out.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write header row if file is empty\n",
    "    # writer.writerow([\"Column1\", \"Column2\"])  # <-- customize columns\n",
    "\n",
    "    # Headers for request (User-Agent + Accept-Language)\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' +\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) ' +\n",
    "                      'Chrome/115.0.5790.98 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")  # or \"html.parser\"\n",
    "\n",
    "    # Example: scrape all <h2> tags\n",
    "    for h2 in soup.find_all(\"h2\"):\n",
    "        text = h2.get_text(strip=True)\n",
    "        print(text)\n",
    "        writer.writerow([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73188548-9bfd-411c-aa57-21b74ac44e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3dd5c8b-3f3a-46c1-8f25-4afb29fe5b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
      "Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.0/4.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.8/4.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.9/4.0 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 4.6 MB/s  0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-6.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4db2d1-a093-4422-8fac-fd733590553b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m webpage = requests.get(URL, headers=HEADERS)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Parse with BeautifulSoup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or \"html.parser\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Example: scrape all <h2> tags\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h2 \u001b[38;5;129;01min\u001b[39;00m soup.find_all(\u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\bs4\\__init__.py:366\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    367\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    370\u001b[39m         )\n\u001b[32m    371\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# URL to scrape\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&dc&qid=1760524556&rnid=8609959031&ref=sr_nr_p_n_condition-type_1&ds=v1%3At9rpJ7fSOGDjM43DA7tb2CR74smyug4Rh2omZo5%2BJJk\" # <-- replace with your target URL\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(\"out.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write header row if file is empty\n",
    "    # writer.writerow([\"Column1\", \"Column2\"])  # <-- customize columns\n",
    "\n",
    "    # Headers for request (User-Agent + Accept-Language)\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' +\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) ' +\n",
    "                      'Chrome/115.0.5790.98 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")  # or \"html.parser\"\n",
    "\n",
    "    # Example: scrape all <h2> tags\n",
    "    for h2 in soup.find_all(\"h2\"):\n",
    "        text = h2.get_text(strip=True)\n",
    "        print(text)\n",
    "        writer.writerow([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb1e01d-a4ec-45cb-9fd9-bcce8f073883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c2002f-2ee0-4655-a56d-b6b235e92684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: lxml\n",
      "Version: 6.0.2\n",
      "Summary: Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API.\n",
      "Home-page: https://lxml.de/\n",
      "Author: lxml dev team\n",
      "Author-email: lxml@lxml.de\n",
      "License: BSD-3-Clause\n",
      "Location: C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd3e1ee-4542-4030-9aae-9f9966ce7646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a180f23-8de9-4175-8227-977857f0f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeed14a0-204d-41c6-a057-1963ecdc13c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m webpage = requests.get(URL, headers=HEADERS)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Parse with BeautifulSoup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or \"html.parser\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Example: scrape all <h2> tags\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h2 \u001b[38;5;129;01min\u001b[39;00m soup.find_all(\u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\bs4\\__init__.py:366\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    367\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    370\u001b[39m         )\n\u001b[32m    371\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# URL to scrape\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&dc&qid=1760524556&rnid=8609959031&ref=sr_nr_p_n_condition-type_1&ds=v1%3At9rpJ7fSOGDjM43DA7tb2CR74smyug4Rh2omZo5%2BJJk\" # <-- replace with your target URL\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(\"out.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write header row if file is empty\n",
    "    # writer.writerow([\"Column1\", \"Column2\"])  # <-- customize columns\n",
    "\n",
    "    # Headers for request (User-Agent + Accept-Language)\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' +\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) ' +\n",
    "                      'Chrome/115.0.5790.98 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")  # or \"html.parser\"\n",
    "\n",
    "    # Example: scrape all <h2> tags\n",
    "    for h2 in soup.find_all(\"h2\"):\n",
    "        text = h2.get_text(strip=True)\n",
    "        print(text)\n",
    "        writer.writerow([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "136d00d4-5502-4b03-97b7-81234951aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# URL to scrape\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&dc&qid=1760524556&rnid=8609959031&ref=sr_nr_p_n_condition-type_1&ds=v1%3At9rpJ7fSOGDjM43DA7tb2CR74smyug4Rh2omZo5%2BJJk\"\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(\"out.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Optional: write headers if starting fresh\n",
    "    writer.writerow([\"Product Title\"])  # you can add more columns later\n",
    "\n",
    "    # Headers for request\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' +\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) ' +\n",
    "                      'Chrome/115.0.5790.98 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Parse with BeautifulSoup using built-in parser\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Amazon product titles are usually inside <span> with class 'a-size-medium a-color-base a-text-normal'\n",
    "    for product in soup.find_all(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"}):\n",
    "        title = product.get_text(strip=True)\n",
    "        print(title)\n",
    "        writer.writerow([title])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d466d3b-0bea-47d2-a68b-ef14a76687f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A 9,898 https://www.amazon.in/realme-Long-Lasting-Resistance-Military-Grade-Durability/dp/B0F9TT7Z5Q/ref=sr_1_1?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.KyTEmyUkiDWVSE96u7Yx_qmXwby47xpTyz-wiJjk2Ok&dib_tag=se&qid=1760525604&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m link_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://www.amazon.in\u001b[39m\u001b[33m\"\u001b[39m + link[\u001b[33m'\u001b[39m\u001b[33mhref\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m link \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(title_text, price_text, link_url)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprice_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_url\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "for product in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "    title = product.find(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "    price = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "    link = product.find(\"a\", {\"class\": \"a-link-normal s-no-outline\"})\n",
    "\n",
    "    title_text = title.get_text(strip=True) if title else \"N/A\"\n",
    "    price_text = price.get_text(strip=True) if price else \"N/A\"\n",
    "    link_url = \"https://www.amazon.in\" + link['href'] if link else \"N/A\"\n",
    "\n",
    "    print(title_text, price_text, link_url)\n",
    "    writer.writerow([title_text, price_text, link_url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3f9456d-4f53-4c6a-8be7-acc712f0dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N/A 9,898 https://www.amazon.in/realme-Long-Lasting-Resistance-Military-Grade-Durability/dp/B0F9TT7Z5Q/ref=sr_1_1?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-1\n",
      "N/A 7,499 https://www.amazon.in/Redmi-A4-5G-Sparkle-Charging/dp/B0DLW427YG/ref=sr_1_2?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-2\n",
      "N/A 10,998 https://www.amazon.in/iQOO-Titanium-Dimensity-Processor-Shock-Resistance/dp/B0FC5TDB9P/ref=sr_1_3?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-3\n",
      "N/A 14,498 https://www.amazon.in/iQOO-Dimensity-Processor-Military-Grade-Durability/dp/B0F2T61RZD/ref=sr_1_4?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-4\n",
      "N/A 10,998 https://www.amazon.in/iQOO-Dimensity-Processor-Military-Shock-Resistance/dp/B0FC5XK9WZ/ref=sr_1_5?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-5\n",
      "N/A 14,498 https://www.amazon.in/iQOO-Ultramarine-Dimensity-Military-Grade-Durability/dp/B0F2T6TV4M/ref=sr_1_6?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-6\n",
      "N/A 9,898 https://www.amazon.in/realme-Long-Lasting-Resistance-Military-Grade-Durability/dp/B0F9TS623W/ref=sr_1_7?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-7\n",
      "N/A 10,999 https://www.amazon.in/Redmi-Diamond-Largest-Display-Segment/dp/B0D78Y577J/ref=sr_1_8?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-8\n",
      "N/A 8,999 https://www.amazon.in/Samsung-MediaTek-Dimensity-Charging-Upgrades/dp/B0DX655V11/ref=sr_1_9?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-9\n",
      "N/A 10,999 https://www.amazon.in/Redmi-Hawaiian-Largest-Display-Segment/dp/B0D78X544X/ref=sr_1_10?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-10\n",
      "N/A 12,998 https://www.amazon.in/iQOO-Dimensity-Processor-Military-Grade-Durability/dp/B0F2T674FJ/ref=sr_1_11?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-11\n",
      "N/A 12,998 https://www.amazon.in/iQOO-Ultramarine-Dimensity-Military-Grade-Durability/dp/B0F2T7B9TM/ref=sr_1_12?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-12\n",
      "N/A 10,999 https://www.amazon.in/Redmi-Orchid-Largest-Display-Segment/dp/B0D78X5CMJ/ref=sr_1_13?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-13\n",
      "N/A 9,998 https://www.amazon.in/iQOO-Titanium-Dimensity-Processor-Shock-Resistance/dp/B0FC5XCLXG/ref=sr_1_14?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-14\n",
      "N/A 8,349 https://www.amazon.in/Redmi-Sparkle-Storage-Segment-Charging/dp/B0DLW44CGS/ref=sr_1_15?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-15\n",
      "N/A 8,999 https://www.amazon.in/Redmi-A4-5G-Sparkle-Charging/dp/B0FBR69B2R/ref=sr_1_16?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-16\n",
      "N/A 13,999 https://www.amazon.in/Samsung-Storage-Enhanced-Unmatched-Nightography/dp/B0FDBB2VRC/ref=sr_1_17?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-17\n",
      "N/A 7,499 https://www.amazon.in/Redmi-Storage-Segment-Largest-Charging/dp/B0DLW1L5PR/ref=sr_1_18?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-18\n",
      "N/A 7,999 https://www.amazon.in/Samsung-MediaTek-Dimensity-Charging-Upgrades/dp/B0DX6P3RX9/ref=sr_1_19?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-19\n",
      "N/A 8,349 https://www.amazon.in/Redmi-A4-5G-Storage-Charging/dp/B0DLW4QD72/ref=sr_1_20?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-20\n",
      "N/A 12,998 https://www.amazon.in/iQOO-Titanium-Dimensity-Processor-Shock-Resistance/dp/B0FC5QGMZ1/ref=sr_1_21?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-21\n",
      "N/A 7,198 https://www.amazon.in/realme-6300mAh-Segments-Biggest-Battery/dp/B0FG2NDHJM/ref=sr_1_22?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-22\n",
      "N/A N/A https://www.amazon.in/iQOO-Dimensity-Processor-Military-Shock-Resistance/dp/B0FC5TBWG5/ref=sr_1_23?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-23\n",
      "N/A 8,999 https://www.amazon.in/Redmi-A4-5G-Storage-Charging/dp/B0FBRM15FH/ref=sr_1_24?dib=eyJ2IjoiMSJ9.7zq4SGsfvqabYxYN7ri7SI4hFrjV_Qc2nBrNcpM_suQFbq2v-_d0Opmh4Vx6kBp9I59G0fPqBiwd8rhqlx4Rm3PoStltkSbLDv6wwJ_cKxvmisDDg2L6niTxuQ62P1z9Y_tKCyTHXQXZay3X1QR95j7J1XyPaqd4WehzbNADP_-rNT8YjxqvkTdtVMB3tthBcKw3nmWGQPNHDEo8t44hc4vIzp-04btruydICQyV1mD9-egRZd3aMbl8Uiafu7zhfNjb7r_LK2Z_rQJ_kt2SxnEcQQPn7xXLfxbexEYqGnw.kWCAOXFTN1YQOgkY4IKw4IRi3u8mke0eQSFwdySLODk&dib_tag=se&qid=1760525964&refinements=p_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&rnid=8609959031&s=electronics&sr=1-24\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# URL to scrape\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&dc&qid=1760524556&rnid=8609959031&ref=sr_nr_p_n_condition-type_1&ds=v1%3At9rpJ7fSOGDjM43DA7tb2CR74smyug4Rh2omZo5%2BJJk\"\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(\"out.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Optional: write headers if starting fresh\n",
    "    writer.writerow([\"Product Title\", \"Price\", \"Link\"])  \n",
    "\n",
    "    # Headers for request\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' +\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) ' +\n",
    "                      'Chrome/115.0.5790.98 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    # Get the webpage\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Loop through products\n",
    "    for product in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        title = product.find(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "        price = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "        link = product.find(\"a\", {\"class\": \"a-link-normal s-no-outline\"})\n",
    "\n",
    "        title_text = title.get_text(strip=True) if title else \"N/A\"\n",
    "        price_text = price.get_text(strip=True) if price else \"N/A\"\n",
    "        link_url = \"https://www.amazon.in\" + link['href'] if link else \"N/A\"\n",
    "\n",
    "        print(title_text, price_text, link_url)\n",
    "        writer.writerow([title_text, price_text, link_url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc4a38c-997b-4d82-bf31-1b8abf66ce16",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'out.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcsv\u001b[39;00m\n\u001b[32m      5\u001b[39m URL = \u001b[33m\"\u001b[39m\u001b[33mhttps://www.amazon.in/s?i=electronics&rh=n\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3A976419031\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2Cp_36\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3A630000-1500000\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2Cp_n_condition-type\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3A8609960031&dc&qid=1760524556\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mout.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      8\u001b[39m     writer = csv.writer(file)\n\u001b[32m      9\u001b[39m     writer.writerow([\u001b[33m\"\u001b[39m\u001b[33mProduct Name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mImage URL\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'out.csv'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031&dc&qid=1760524556\"\n",
    "\n",
    "with open(\"out.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Product Name\", \"Price\", \"Link\", \"Image URL\"])\n",
    "\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' +\n",
    "                      '(KHTML, like Gecko) Chrome/115.0.5790.98 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    for product in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        title = product.find(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "        price = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "        link = product.find(\"a\", {\"class\": \"a-link-normal s-no-outline\"})\n",
    "        image = product.find(\"img\", {\"class\": \"s-image\"})\n",
    "\n",
    "        title_text = title.get_text(strip=True) if title else \"N/A\"\n",
    "        price_text = price.get_text(strip=True) if price else \"N/A\"\n",
    "        link_url = \"https://www.amazon.in\" + link['href'] if link else \"N/A\"\n",
    "        image_url = image['src'] if image else \"N/A\"\n",
    "\n",
    "        print(title_text, price_text, link_url, image_url)\n",
    "        writer.writerow([title_text, price_text, link_url, image_url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88005c37-ad04-49f3-875b-2dcd081928e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwebdriver\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchrome\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwebdriver\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m By\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "\n",
    "# Setup Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(\"chromedriver.exe\"))\n",
    "\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031\"\n",
    "driver.get(URL)\n",
    "\n",
    "with open(\"out.csv\", \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Price\", \"Link\", \"Image\"])\n",
    "\n",
    "    products = driver.find_elements(By.XPATH, '//div[@data-component-type=\"s-search-result\"]')\n",
    "    for p in products:\n",
    "        try:\n",
    "            title = p.find_element(By.XPATH, './/span[@class=\"a-size-medium a-color-base a-text-normal\"]').text\n",
    "        except:\n",
    "            title = \"N/A\"\n",
    "        try:\n",
    "            price = p.find_element(By.XPATH, './/span[@class=\"a-price-whole\"]').text\n",
    "        except:\n",
    "            price = \"N/A\"\n",
    "        try:\n",
    "            link = p.find_element(By.XPATH, './/a[@class=\"a-link-normal s-no-outline\"]').get_attribute(\"href\")\n",
    "        except:\n",
    "            link = \"N/A\"\n",
    "        try:\n",
    "            image = p.find_element(By.XPATH, './/img').get_attribute(\"src\")\n",
    "        except:\n",
    "            image = \"N/A\"\n",
    "        writer.writerow([title, price, link, image])\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adcaab10-639b-4e25-a30c-2d21f7f001c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31b4ab91-13c7-47a3-8e51-f2a377742ce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (4202517002.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwith open(r\"C:\\Users\\Himesh\\Documents\\out.csv\", \"w\", newline='', encoding=\"utf-8\") as file:\u001b[39m\n                                                                                               ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "with open(r\"C:\\Users\\Himesh\\Documents\\out.csv\", \"w\", newline='', encoding=\"utf-8\") as file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9ed945c-c40f-4272-800a-37c235d89eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Collecting trio<1.0,>=0.30.0 (from selenium)\n",
      "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (25.4.0)\n",
      "Collecting sortedcontainers (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.11)\n",
      "Collecting outcome (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.0.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 9.6/9.6 MB 49.2 MB/s  0:00:00\n",
      "Downloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, pysocks, outcome, trio, trio-websocket, selenium\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [wsproto]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------- ----------------- 4/7 [trio]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------- ----- 6/7 [selenium]\n",
      "   ---------------------------------------- 7/7 [selenium]\n",
      "\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.36.0 sortedcontainers-2.4.0 trio-0.31.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b930761-6858-4f92-8c5f-ebdbcda9b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51137bc1-2676-469b-92b0-5e69634c297d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:64\u001b[39m, in \u001b[36mDriverFinder._binary_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path).is_file():\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m._paths[\u001b[33m\"\u001b[39m\u001b[33mdriver_path\u001b[39m\u001b[33m\"\u001b[39m] = path\n",
      "\u001b[31mValueError\u001b[39m: The path is not a valid file: chromedriver.exe",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNoSuchDriverException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcsv\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Setup Chrome driver\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m driver = \u001b[43mwebdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mService\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchromedriver.exe\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m URL = \u001b[33m\"\u001b[39m\u001b[33mhttps://www.amazon.in/s?i=electronics&rh=n\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3A976419031\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2Cp_36\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3A630000-1500000\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2Cp_n_condition-type\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3A8609960031\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m driver.get(URL)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:46\u001b[39m, in \u001b[36mWebDriver.__init__\u001b[39m\u001b[34m(self, options, service, keep_alive)\u001b[39m\n\u001b[32m     43\u001b[39m service = service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[32m     44\u001b[39m options = options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbrowserName\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:55\u001b[39m, in \u001b[36mChromiumDriver.__init__\u001b[39m\u001b[34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[39m\n\u001b[32m     52\u001b[39m options = options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m ChromiumOptions()\n\u001b[32m     54\u001b[39m finder = DriverFinder(\u001b[38;5;28mself\u001b[39m.service, options)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     56\u001b[39m     options.binary_location = finder.get_browser_path()\n\u001b[32m     57\u001b[39m     options.browser_version = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[39m, in \u001b[36mDriverFinder.get_browser_path\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mbrowser_path\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:78\u001b[39m, in \u001b[36mDriverFinder._binary_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     77\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paths\n",
      "\u001b[31mNoSuchDriverException\u001b[39m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "\n",
    "# Setup Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(\"chromedriver.exe\"))\n",
    "\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031\"\n",
    "driver.get(URL)\n",
    "\n",
    "with open(\"out.csv\", \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Price\", \"Link\", \"Image\"])\n",
    "\n",
    "    products = driver.find_elements(By.XPATH, '//div[@data-component-type=\"s-search-result\"]')\n",
    "    for p in products:\n",
    "        try:\n",
    "            title = p.find_element(By.XPATH, './/span[@class=\"a-size-medium a-color-base a-text-normal\"]').text\n",
    "        except:\n",
    "            title = \"N/A\"\n",
    "        try:\n",
    "            price = p.find_element(By.XPATH, './/span[@class=\"a-price-whole\"]').text\n",
    "        except:\n",
    "            price = \"N/A\"\n",
    "        try:\n",
    "            link = p.find_element(By.XPATH, './/a[@class=\"a-link-normal s-no-outline\"]').get_attribute(\"href\")\n",
    "        except:\n",
    "            link = \"N/A\"\n",
    "        try:\n",
    "            image = p.find_element(By.XPATH, './/img').get_attribute(\"src\")\n",
    "        except:\n",
    "            image = \"N/A\"\n",
    "        writer.writerow([title, price, link, image])\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38e0398c-8967-4136-9f13-c276c0e4d445",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3014536230.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mValueError: The path is not a valid file: chromedriver.exe\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ValueError: The path is not a valid file: chromedriver.exe\n",
    "NoSuchDriverException: Unable to obtain driver for chrome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fc0dacb-bb4d-4da6-b6d6-bf2b178a4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.140 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")  # no lxml needed\n",
    "\n",
    "with open(\"out.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Price\", \"Link\", \"Image\"])\n",
    "\n",
    "    for product in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        title = product.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\")\n",
    "        price = product.find(\"span\", class_=\"a-price-whole\")\n",
    "        link = product.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "        image = product.find(\"img\", class_=\"s-image\")\n",
    "\n",
    "        writer.writerow([\n",
    "            title.get_text(strip=True) if title else \"N/A\",\n",
    "            price.get_text(strip=True) if price else \"N/A\",\n",
    "            \"https://www.amazon.in\" + link['href'] if link else \"N/A\",\n",
    "            image['src'] if image else \"N/A\"\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef3a69d9-0290-4c82-a378-41ebc31b1639",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Himesh\\\\Desktop\\\\out.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m response = requests.get(URL, headers=HEADERS)\n\u001b[32m     14\u001b[39m soup = BeautifulSoup(response.content, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mHimesh\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mout.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     17\u001b[39m     writer = csv.writer(file)\n\u001b[32m     18\u001b[39m     writer.writerow([\u001b[33m\"\u001b[39m\u001b[33mTitle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mImage\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Himesh\\\\Desktop\\\\out.csv'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" +\n",
    "                  \"(KHTML, like Gecko) Chrome/116.0.5845.140 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "with open(r\"C:\\Users\\Himesh\\Desktop\\out.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Price\", \"Link\", \"Image\"])\n",
    "\n",
    "    for product in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        # Try multiple ways to get product name\n",
    "        title_tag = product.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\")\n",
    "        if not title_tag:\n",
    "            title_tag = product.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "            title_text = title_tag['title'] if title_tag and title_tag.has_attr('title') else \"N/A\"\n",
    "        else:\n",
    "            title_text = title_tag.get_text(strip=True)\n",
    "        \n",
    "        price_tag = product.find(\"span\", class_=\"a-price-whole\")\n",
    "        link_tag = product.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "        image_tag = product.find(\"img\", class_=\"s-image\")\n",
    "\n",
    "        writer.writerow([\n",
    "            title_text,\n",
    "            price_tag.get_text(strip=True) if price_tag else \"N/A\",\n",
    "            \"https://www.amazon.in\" + link_tag['href'] if link_tag else \"N/A\",\n",
    "            image_tag['src'] if image_tag else \"N/A\"\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f6432ac-63f4-4bee-b264-6cd4b4f7fa34",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Himesh\\\\Desktop\\\\out.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m response = requests.get(URL, headers=HEADERS)\n\u001b[32m     14\u001b[39m soup = BeautifulSoup(response.content, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mHimesh\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mout.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     17\u001b[39m     writer = csv.writer(file)\n\u001b[32m     18\u001b[39m     writer.writerow([\u001b[33m\"\u001b[39m\u001b[33mTitle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mImage\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Himesh\\\\Desktop\\\\out.csv'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" +\n",
    "                  \"(KHTML, like Gecko) Chrome/116.0.5845.140 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "with open(r\"C:\\Users\\Himesh\\Desktop\\out.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Price\", \"Link\", \"Image\"])\n",
    "\n",
    "    for product in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        # Try multiple ways to get product name\n",
    "        title_tag = product.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\")\n",
    "        if not title_tag:\n",
    "            title_tag = product.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "            title_text = title_tag['title'] if title_tag and title_tag.has_attr('title') else \"N/A\"\n",
    "        else:\n",
    "            title_text = title_tag.get_text(strip=True)\n",
    "        \n",
    "        price_tag = product.find(\"span\", class_=\"a-price-whole\")\n",
    "        link_tag = product.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "        image_tag = product.find(\"img\", class_=\"s-image\")\n",
    "\n",
    "        writer.writerow([\n",
    "            title_text,\n",
    "            price_tag.get_text(strip=True) if price_tag else \"N/A\",\n",
    "            \"https://www.amazon.in\" + link_tag['href'] if link_tag else \"N/A\",\n",
    "            image_tag['src'] if image_tag else \"N/A\"\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78150b69-72a0-4333-a6c6-7bf9f30170de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! CSV file saved as out.csv in the current folder.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# Amazon URL to scrape\n",
    "URL = \"https://www.amazon.in/s?i=electronics&rh=n%3A976419031%2Cp_36%3A630000-1500000%2Cp_n_condition-type%3A8609960031\"\n",
    "\n",
    "# Headers to mimic a real browser\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/116.0.5845.140 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Open CSV file in write mode\n",
    "with open(\"out.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Product Name\", \"Price\", \"Link\", \"Image URL\"])  # header row\n",
    "\n",
    "    # Loop through all products\n",
    "    for product in soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        # Get product title\n",
    "        title = product.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\")\n",
    "        # Fallback: sometimes title is inside h2 tag if class changes\n",
    "        if not title:\n",
    "            title_tag = product.find(\"h2\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "        else:\n",
    "            title = title.get_text(strip=True)\n",
    "        \n",
    "        # Get product price\n",
    "        price = product.find(\"span\", class_=\"a-price-whole\")\n",
    "        price = price.get_text(strip=True) if price else \"N/A\"\n",
    "\n",
    "        # Get product link\n",
    "        link_tag = product.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "        link = \"https://www.amazon.in\" + link_tag['href'] if link_tag else \"N/A\"\n",
    "\n",
    "        # Get product image\n",
    "        image_tag = product.find(\"img\", class_=\"s-image\")\n",
    "        image = image_tag['src'] if image_tag else \"N/A\"\n",
    "\n",
    "        # Write row in CSV\n",
    "        writer.writerow([title, price, link, image])\n",
    "\n",
    "print(\"Scraping complete! CSV file saved as out.csv in the current folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd71fb-24b7-46bb-bb69-e3884f5958d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
